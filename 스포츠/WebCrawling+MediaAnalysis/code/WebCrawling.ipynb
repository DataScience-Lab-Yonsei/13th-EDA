{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case somebody is gonna use the same code, my code is shit, but it works. First import those libs, and run the first block in a competition's section, after it downloaded, manually delete useless chaaracters and save it. Then run the second block, it will add the column index to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pair data（completed）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: C:\\Users\\a normal person\\OneDrive\\桌面\\yonsei\\data science lab\\13기 활동\\EDA project\\dataset\n",
      "Output file: C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\pairs_scores.csv\n",
      "Data successfully written to C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\pairs_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# URL of the target page\n",
    "url = \"https://skatingscores.com/pairs/\"\n",
    "\n",
    "# Specify the storage path for the CSV file\n",
    "output_dir = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"  # Use relative path or absolute path here\n",
    "print(f\"Output directory: {os.path.abspath(output_dir)}\")  # Debug: Print full path\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    print(f\"Directory {output_dir} does not exist. Creating it now...\")\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"pairs_scores.csv\")\n",
    "print(f\"Output file: {output_path}\")  # Debug: Print output file path\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the website\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing pairs scores\n",
    "    table = soup.find(\"table\")  # Assuming there's only one main table on the page\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Open a CSV file to write the data\n",
    "        with open(output_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Write the data rows\n",
    "            for row in rows[1:]:\n",
    "                columns = [td.text.strip() for td in row.find_all(\"td\")]\n",
    "                writer.writerow(columns)\n",
    "\n",
    "        print(f\"Data successfully written to {output_path}\")\n",
    "    else:\n",
    "        print(\"Table not found on the page.\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Failed to connect to the URL. Please check your internet connection or the URL.\")\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"The request timed out. Try again later.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# 读取你的 CSV 文件\n",
    "file_path = '..\\dataset\\pairs_scores.csv' \n",
    "df_pair = pd.read_csv(file_path, header=None) \n",
    "\n",
    "# 新的列名列表\n",
    "new_columns = [\n",
    "    'seq','Nat', 'Team', 'JGP', 'CS', 'GP', 'Ch', 'WD',\n",
    "    'Mean Event Total', 'Mean Event BV', 'Mean Event TES',\n",
    "    'Mean Event PCS', 'Mean SBS Jump', 'Mean Throw',\n",
    "    'Mean Twist', 'Mean Lift', 'Mean CO', 'Mean PR', 'Mean SS'\n",
    "]\n",
    "\n",
    "df_pair.columns = new_columns\n",
    "\n",
    "df_pair.to_csv('pairs_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dancing data(completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: C:\\Users\\a normal person\\OneDrive\\桌面\\yonsei\\data science lab\\13기 활동\\EDA project\\dataset\n",
      "Output file: C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\dance_scores.csv\n",
      "Data successfully written to C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\dance_scores.csv\n",
      "18\n",
      "Number of columns: 18\n"
     ]
    }
   ],
   "source": [
    "# URL of the target page\n",
    "url = \"https://skatingscores.com/dance/\"\n",
    "\n",
    "# Specify the storage path for the CSV file\n",
    "output_dir = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"  # Use relative path or absolute path here\n",
    "print(f\"Output directory: {os.path.abspath(output_dir)}\")  # Debug: Print full path\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    print(f\"Directory {output_dir} does not exist. Creating it now...\")\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"dance_scores.csv\")\n",
    "print(f\"Output file: {output_path}\")  # Debug: Print output file path\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the website\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing pairs scores\n",
    "    table = soup.find(\"table\")  # Assuming there's only one main table on the page\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Open a CSV file to write the data\n",
    "        with open(output_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Write the data rows\n",
    "            for row in rows[1:]:\n",
    "                columns = [td.text.strip() for td in row.find_all(\"td\")]\n",
    "                writer.writerow(columns)\n",
    "\n",
    "        print(f\"Data successfully written to {output_path}\")\n",
    "    else:\n",
    "        print(\"Table not found on the page.\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Failed to connect to the URL. Please check your internet connection or the URL.\")\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"The request timed out. Try again later.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "file_path = '../dataset/dance_scores.csv' \n",
    "df_dance = pd.read_csv(file_path, header=None) \n",
    "\n",
    "new_columns = [\n",
    "    'index','Nat', 'Team', 'JGP', 'CS', 'GP', 'Ch', 'WD',\n",
    "    'Mean Event Total', 'Mean Event BV', 'Mean Event TES',\n",
    "    'Mean Event PCS', 'Mean Twizzle', 'Mean Step Seq',\n",
    "    'Mean Chor Seq', 'Mean CO', 'Mean PR', 'Mean SS'\n",
    "]\n",
    "print(len(new_columns))\n",
    "print(f\"Number of columns: {df_dance.shape[1]}\")\n",
    "final_path = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"\n",
    "df_dance.columns = new_columns\n",
    "df_dance.to_csv('dance_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Men dataset(completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: C:\\Users\\a normal person\\OneDrive\\桌面\\yonsei\\data science lab\\13기 활동\\EDA project\\dataset\n",
      "Output file: C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\men_scores.csv\n",
      "Data successfully written to C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\men_scores.csv\n",
      "19\n",
      "Number of columns: 19\n"
     ]
    }
   ],
   "source": [
    "# URL of the target page\n",
    "url = \"https://skatingscores.com/men/\"\n",
    "\n",
    "# Specify the storage path for the CSV file\n",
    "output_dir = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"  # Use relative path or absolute path here\n",
    "print(f\"Output directory: {os.path.abspath(output_dir)}\")  \n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    print(f\"Directory {output_dir} does not exist. Creating it now...\")\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"men_scores.csv\")\n",
    "print(f\"Output file: {output_path}\")  # Debug: Print output file path\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the website\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing pairs scores\n",
    "    table = soup.find(\"table\")  # Assuming there's only one main table on the page\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Open a CSV file to write the data\n",
    "        with open(output_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Write the data rows\n",
    "            for row in rows[1:]:\n",
    "                columns = [td.text.strip() for td in row.find_all(\"td\")]\n",
    "                writer.writerow(columns)\n",
    "\n",
    "        print(f\"Data successfully written to {output_path}\")\n",
    "    else:\n",
    "        print(\"Table not found on the page.\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Failed to connect to the URL. Please check your internet connection or the URL.\")\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"The request timed out. Try again later.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "file_path = '../dataset/men_scores.csv' \n",
    "df_men = pd.read_csv(file_path, header=None) \n",
    "new_columns = [\n",
    "    'index','Nat', 'Skater', 'JGP', 'CS', 'GP', 'Ch', 'WD',\n",
    "    'Mean Event Total', 'Mean Event BV', 'Mean Event TES',\n",
    "    'Mean Event PCS', 'Mean Jump', 'Mean Spin',\n",
    "    'Mean Step Seq', 'Mean Chor Seq', 'Mean CO', 'Mean PR', 'Mean SS'\n",
    "]\n",
    "print(len(new_columns))\n",
    "print(f\"Number of columns: {df_men.shape[1]}\")\n",
    "final_path = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"\n",
    "df_men.columns = new_columns\n",
    "df_men.to_csv('men_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: C:\\Users\\a normal person\\OneDrive\\桌面\\yonsei\\data science lab\\13기 활동\\EDA project\\dataset\n",
      "Output file: C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\women_scores.csv\n",
      "Data successfully written to C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\women_scores.csv\n",
      "19\n",
      "Number of columns: 19\n"
     ]
    }
   ],
   "source": [
    "# URL of the target page\n",
    "url = \"https://skatingscores.com/women/\"\n",
    "\n",
    "# Specify the storage path for the CSV file\n",
    "output_dir = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"  # Use relative path or absolute path here\n",
    "print(f\"Output directory: {os.path.abspath(output_dir)}\")  \n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    print(f\"Directory {output_dir} does not exist. Creating it now...\")\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"women_scores.csv\")\n",
    "print(f\"Output file: {output_path}\")  # Debug: Print output file path\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the website\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing pairs scores\n",
    "    table = soup.find(\"table\")  # Assuming there's only one main table on the page\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Open a CSV file to write the data\n",
    "        with open(output_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Write the data rows\n",
    "            for row in rows[1:]:\n",
    "                columns = [td.text.strip() for td in row.find_all(\"td\")]\n",
    "                writer.writerow(columns)\n",
    "\n",
    "        print(f\"Data successfully written to {output_path}\")\n",
    "    else:\n",
    "        print(\"Table not found on the page.\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Failed to connect to the URL. Please check your internet connection or the URL.\")\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"The request timed out. Try again later.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "file_path = '../dataset/women_scores.csv' \n",
    "df_women = pd.read_csv(file_path, header=None) \n",
    "new_columns = [\n",
    "    'index','Nat', 'Skater', 'JGP', 'CS', 'GP', 'Ch', 'WD',\n",
    "    'Mean Event Total', 'Mean Event BV', 'Mean Event TES',\n",
    "    'Mean Event PCS', 'Mean Jump', 'Mean Spin',\n",
    "    'Mean Step Seq', 'Mean Chor Seq', 'Mean CO', 'Mean PR', 'Mean SS'\n",
    "]\n",
    "print(len(new_columns))\n",
    "print(f\"Number of columns: {df_women.shape[1]}\")\n",
    "final_path = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"\n",
    "df_women.columns = new_columns\n",
    "df_women.to_csv('women_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vancouver 2010 short program data(completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: C:\\Users\\a normal person\\OneDrive\\桌面\\yonsei\\data science lab\\13기 활동\\EDA project\\dataset\n",
      "Output file: C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\women_scores.csv\n",
      "Data successfully written to C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\women_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# URL of the target page\n",
    "url = \"https://www.isuresults.com/results/owg2010/SEG003.HTM\"\n",
    "\n",
    "# Specify the storage path for the CSV file\n",
    "output_dir = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"  # Use relative path or absolute path here\n",
    "print(f\"Output directory: {os.path.abspath(output_dir)}\")  \n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    print(f\"Directory {output_dir} does not exist. Creating it now...\")\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"Vancouver2010FemaleShortProgram.csv\")\n",
    "print(f\"Output file: {output_path}\")  # Debug: Print output file path\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the website\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing pairs scores\n",
    "    table = soup.find(\"table\")  # Assuming there's only one main table on the page\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Open a CSV file to write the data\n",
    "        with open(output_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Write the data rows\n",
    "            for row in rows[1:]:\n",
    "                columns = [td.text.strip() for td in row.find_all(\"td\")]\n",
    "                writer.writerow(columns)\n",
    "\n",
    "        print(f\"Data successfully written to {output_path}\")\n",
    "    else:\n",
    "        print(\"Table not found on the page.\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Failed to connect to the URL. Please check your internet connection or the URL.\")\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"The request timed out. Try again later.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Number of columns: 14\n"
     ]
    }
   ],
   "source": [
    "file_path = '../dataset/Vancouver2010FemaleShortProgram.csv' \n",
    "df_Van2010FemaleShortProgram= pd.read_csv(file_path, header=None) \n",
    "new_columns = [\n",
    "    'Pl', 'Name', 'Nation', 'TSS', 'TES','', 'PCS',\n",
    "    'SS', 'TR', 'PE', 'CH', 'IN', 'Ded', 'StN'\n",
    "]\n",
    "\n",
    "print(len(new_columns))\n",
    "print(f\"Number of columns: {df_Van2010FemaleShortProgram.shape[1]}\")\n",
    "final_path = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"\n",
    "df_Van2010FemaleShortProgram.columns = new_columns\n",
    "df_Van2010FemaleShortProgram.to_csv('Vancouver2010FemaleShortProgram.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vancouver 2010 free skating data(completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: C:\\Users\\a normal person\\OneDrive\\桌面\\yonsei\\data science lab\\13기 활동\\EDA project\\dataset\n",
      "Output file: C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\Vancouver2010FemaleFreeSkating.csv\n",
      "Data successfully written to C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\Vancouver2010FemaleFreeSkating.csv\n"
     ]
    }
   ],
   "source": [
    "# URL of the target page\n",
    "url = \"https://www.isuresults.com/results/owg2010/SEG004.HTM\"\n",
    "\n",
    "# Specify the storage path for the CSV file\n",
    "output_dir = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"  # Use relative path or absolute path here\n",
    "print(f\"Output directory: {os.path.abspath(output_dir)}\")  \n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    print(f\"Directory {output_dir} does not exist. Creating it now...\")\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"Vancouver2010FemaleFreeSkating.csv\")\n",
    "print(f\"Output file: {output_path}\")  # Debug: Print output file path\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the website\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing pairs scores\n",
    "    table = soup.find(\"table\")  # Assuming there's only one main table on the page\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Open a CSV file to write the data\n",
    "        with open(output_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Write the data rows\n",
    "            for row in rows[1:]:\n",
    "                columns = [td.text.strip() for td in row.find_all(\"td\")]\n",
    "                writer.writerow(columns)\n",
    "\n",
    "        print(f\"Data successfully written to {output_path}\")\n",
    "    else:\n",
    "        print(\"Table not found on the page.\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Failed to connect to the URL. Please check your internet connection or the URL.\")\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"The request timed out. Try again later.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Number of columns: 14\n"
     ]
    }
   ],
   "source": [
    "file_path = '../dataset/Vancouver2010FemaleFreeSkating.csv' \n",
    "df_Van2010FemaleFreeSkating= pd.read_csv(file_path, header=None) \n",
    "new_columns = [\n",
    "    'Pl', 'Name', 'Nation', 'TSS', 'TES','', 'PCS',\n",
    "    'SS', 'TR', 'PE', 'CH', 'IN', 'Ded', 'StN'\n",
    "]\n",
    "\n",
    "print(len(new_columns))\n",
    "print(f\"Number of columns: {df_Van2010FemaleFreeSkating.shape[1]}\")\n",
    "final_path = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"\n",
    "df_Van2010FemaleFreeSkating.columns = new_columns\n",
    "df_Van2010FemaleFreeSkating.to_csv('Vancouver2010FemaleFreeSkating.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vancouver 2014 short program data(completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: C:\\Users\\a normal person\\OneDrive\\桌面\\yonsei\\data science lab\\13기 활동\\EDA project\\dataset\n",
      "Output file: C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\Vancouver2014FemaleShortProgram.csv\n",
      "Data successfully written to C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\Vancouver2014FemaleShortProgram.csv\n"
     ]
    }
   ],
   "source": [
    "# URL of the target page\n",
    "url = \"https://www.isuresults.com/results/owg2014/SEG003.HTM\"\n",
    "\n",
    "# Specify the storage path for the CSV file\n",
    "output_dir = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"  # Use relative path or absolute path here\n",
    "print(f\"Output directory: {os.path.abspath(output_dir)}\")  \n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    print(f\"Directory {output_dir} does not exist. Creating it now...\")\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"Vancouver2014FemaleShortProgram.csv\")\n",
    "print(f\"Output file: {output_path}\")  # Debug: Print output file path\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the website\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing pairs scores\n",
    "    table = soup.find(\"table\")  # Assuming there's only one main table on the page\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Open a CSV file to write the data\n",
    "        with open(output_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Write the data rows\n",
    "            for row in rows[1:]:\n",
    "                columns = [td.text.strip() for td in row.find_all(\"td\")]\n",
    "                writer.writerow(columns)\n",
    "\n",
    "        print(f\"Data successfully written to {output_path}\")\n",
    "    else:\n",
    "        print(\"Table not found on the page.\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Failed to connect to the URL. Please check your internet connection or the URL.\")\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"The request timed out. Try again later.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Number of columns: 15\n"
     ]
    }
   ],
   "source": [
    "file_path = '../dataset/Vancouver2014FemaleshortProgram.csv' \n",
    "df_Van2014FemaleshortProgram= pd.read_csv(file_path, header=None) \n",
    "new_columns = [\n",
    "    'Pl', 'Qual', 'Name', 'Nation', 'TSS', 'TES','', 'PCS',\n",
    "    'SS', 'TR', 'PE', 'CH', 'IN', 'Ded', 'StN'\n",
    "]\n",
    "\n",
    "\n",
    "print(len(new_columns))\n",
    "print(f\"Number of columns: {df_Van2014FemaleshortProgram.shape[1]}\")\n",
    "final_path = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"\n",
    "df_Van2014FemaleshortProgram.columns = new_columns\n",
    "df_Van2014FemaleshortProgram.to_csv('Vancouver2014FemaleshortProgram.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vancouver 2014 free skating data(completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: C:\\Users\\a normal person\\OneDrive\\桌面\\yonsei\\data science lab\\13기 활동\\EDA project\\dataset\n",
      "Output file: C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\Vancouver2014FemaleFreeSkating.csv\n",
      "Data successfully written to C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\Vancouver2014FemaleFreeSkating.csv\n"
     ]
    }
   ],
   "source": [
    "# URL of the target page\n",
    "url = \"https://www.isuresults.com/results/owg2014/SEG004.HTM\"\n",
    "\n",
    "# Specify the storage path for the CSV file\n",
    "output_dir = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"  # Use relative path or absolute path here\n",
    "print(f\"Output directory: {os.path.abspath(output_dir)}\")  \n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    print(f\"Directory {output_dir} does not exist. Creating it now...\")\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"Vancouver2014FemaleFreeSkating.csv\")\n",
    "print(f\"Output file: {output_path}\")  # Debug: Print output file path\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the website\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing pairs scores\n",
    "    table = soup.find(\"table\")  # Assuming there's only one main table on the page\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Open a CSV file to write the data\n",
    "        with open(output_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Write the data rows\n",
    "            for row in rows[1:]:\n",
    "                columns = [td.text.strip() for td in row.find_all(\"td\")]\n",
    "                writer.writerow(columns)\n",
    "\n",
    "        print(f\"Data successfully written to {output_path}\")\n",
    "    else:\n",
    "        print(\"Table not found on the page.\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Failed to connect to the URL. Please check your internet connection or the URL.\")\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"The request timed out. Try again later.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Number of columns: 14\n"
     ]
    }
   ],
   "source": [
    "file_path = '../dataset/Vancouver2014FemaleFreeSkating.csv' \n",
    "df_Van2014FemaleFreeSkating= pd.read_csv(file_path, header=None) \n",
    "new_columns = [\n",
    "    'Pl', 'Name', 'Nation', 'TSS', 'TES','', 'PCS',\n",
    "    'SS', 'TR', 'PE', 'CH', 'IN', 'Ded', 'StN'\n",
    "]\n",
    "\n",
    "\n",
    "print(len(new_columns))\n",
    "print(f\"Number of columns: {df_Van2014FemaleFreeSkating.shape[1]}\")\n",
    "final_path = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"\n",
    "df_Van2014FemaleFreeSkating.columns = new_columns\n",
    "df_Van2014FemaleFreeSkating.to_csv('Vancouver2014FemaleFreeSkating.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyeong Chang 2018 Short Program data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: C:\\Users\\a normal person\\OneDrive\\桌面\\yonsei\\data science lab\\13기 활동\\EDA project\\dataset\n",
      "Output file: C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\PyeongChang2018ShortProgramData.csv\n",
      "Data successfully written to C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\PyeongChang2018ShortProgramData.csv\n"
     ]
    }
   ],
   "source": [
    "# URL of the target page\n",
    "url = \"https://www.isuresults.com/results/season1718/owg2018/SEG003.HTM\"\n",
    "\n",
    "# Specify the storage path for the CSV file\n",
    "output_dir = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"  # Use relative path or absolute path here\n",
    "print(f\"Output directory: {os.path.abspath(output_dir)}\")  \n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    print(f\"Directory {output_dir} does not exist. Creating it now...\")\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"PyeongChang2018ShortProgramData.csv\")\n",
    "print(f\"Output file: {output_path}\")  # Debug: Print output file path\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the website\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing pairs scores\n",
    "    table = soup.find(\"table\")  # Assuming there's only one main table on the page\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Open a CSV file to write the data\n",
    "        with open(output_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Write the data rows\n",
    "            for row in rows[1:]:\n",
    "                columns = [td.text.strip() for td in row.find_all(\"td\")]\n",
    "                writer.writerow(columns)\n",
    "\n",
    "        print(f\"Data successfully written to {output_path}\")\n",
    "    else:\n",
    "        print(\"Table not found on the page.\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Failed to connect to the URL. Please check your internet connection or the URL.\")\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"The request timed out. Try again later.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Number of columns: 15\n"
     ]
    }
   ],
   "source": [
    "file_path = '../dataset/PyeongChang2018ShortProgramData.csv' \n",
    "df_Van2014FemaleFreeSkating= pd.read_csv(file_path, header=None) \n",
    "new_columns = [\n",
    "    'Pl', 'Qual','Name', 'Nation', 'TSS', 'TES','', 'PCS',\n",
    "    'SS', 'TR', 'PE', 'CO', 'IN', 'Ded', 'StN'\n",
    "]\n",
    "\n",
    "\n",
    "print(len(new_columns))\n",
    "print(f\"Number of columns: {df_Van2014FemaleFreeSkating.shape[1]}\")\n",
    "final_path = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"\n",
    "df_Van2014FemaleFreeSkating.columns = new_columns\n",
    "df_Van2014FemaleFreeSkating.to_csv('PyeongChang2018ShortProgramData.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyeong Chang 2018 Free Skating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: C:\\Users\\a normal person\\OneDrive\\桌面\\yonsei\\data science lab\\13기 활동\\EDA project\\dataset\n",
      "Output file: C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\PyeongChang2018FreeSkatingData.csv\n",
      "Data successfully written to C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\\PyeongChang2018FreeSkatingData.csv\n"
     ]
    }
   ],
   "source": [
    "# URL of the target page\n",
    "url = \"https://www.isuresults.com/results/season1718/owg2018/SEG004.HTM\"\n",
    "\n",
    "# Specify the storage path for the CSV file\n",
    "output_dir = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"  # Use relative path or absolute path here\n",
    "print(f\"Output directory: {os.path.abspath(output_dir)}\")  \n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    print(f\"Directory {output_dir} does not exist. Creating it now...\")\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"PyeongChang2018FreeSkatingData.csv\")\n",
    "print(f\"Output file: {output_path}\")  # Debug: Print output file path\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the website\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing pairs scores\n",
    "    table = soup.find(\"table\")  # Assuming there's only one main table on the page\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Open a CSV file to write the data\n",
    "        with open(output_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Write the data rows\n",
    "            for row in rows[1:]:\n",
    "                columns = [td.text.strip() for td in row.find_all(\"td\")]\n",
    "                writer.writerow(columns)\n",
    "\n",
    "        print(f\"Data successfully written to {output_path}\")\n",
    "    else:\n",
    "        print(\"Table not found on the page.\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Failed to connect to the URL. Please check your internet connection or the URL.\")\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"The request timed out. Try again later.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Number of columns: 14\n"
     ]
    }
   ],
   "source": [
    "file_path = '../dataset/PyeongChang2018FreeSkatingData.csv' \n",
    "df_Van2014FemaleFreeSkating= pd.read_csv(file_path, header=None) \n",
    "new_columns = [\n",
    "    'Pl','Name', 'Nation', 'TSS', 'TES','', 'PCS',\n",
    "    'SS', 'TR', 'PE', 'CO', 'IN', 'Ded', 'StN'\n",
    "]\n",
    "\n",
    "\n",
    "print(len(new_columns))\n",
    "print(f\"Number of columns: {df_Van2014FemaleFreeSkating.shape[1]}\")\n",
    "final_path = \"C:/Users/a normal person/OneDrive/桌面/yonsei/data science lab/13기 활동/EDA project/dataset\"\n",
    "df_Van2014FemaleFreeSkating.columns = new_columns\n",
    "df_Van2014FemaleFreeSkating.to_csv('PyeongChang2018FreeSkatingData.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
